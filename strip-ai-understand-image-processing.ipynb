{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f90c1ff0",
   "metadata": {},
   "source": [
    "## Mayo Clinic - STRIP AI -  Understanding image processing \n",
    "\n",
    "use `PIL` (pillow package) and `torchvision` to load and process images.\n",
    "\n",
    "- Get image metadata\n",
    "    - get file size and create/update timestamps via `pathlib`\n",
    "    - get image metadata via `PIL` package\n",
    "        - image lenght, width, mode, and so on\n",
    "- resize images `PIL` package\n",
    "    - use `PIL` thumbnail to resize images while keeping the original image height/width ratio\n",
    "    - note that when converting `PIL` object to numpy, the data is in `[0, 255]` not `[0, 1]`\n",
    "- crop and pad images by `torchvision` tranforms\n",
    "    - use `torchvision` to crop and pad images\n",
    "    - **crop** image: \n",
    "        - when the original size is 512*480, and by cropping the image to 512, the new image will be 512*512, and the additional area is filled with 0 (shown as black)\n",
    "        - when the original size is 512*480 and by cropping the image to 480, the new image will be 480*480\n",
    "    - **pad** image: \n",
    "        - when the original size is 512*480, and by padding the image by 10, the new image will be 522*490, the addtional area is filled with 0 (shown as black)\n",
    "- add guassion blur to images by `torchvision` tranforms\n",
    "- normalize images\n",
    "    - note that before \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e0a7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "import gc\n",
    "import copy\n",
    "\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "\n",
    " \n",
    "from dateutil.relativedelta import relativedelta\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "\n",
    "pd.options.display.max_rows = 100\n",
    "pd.options.display.max_columns = 100\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "random_seed=1234\n",
    "pl.seed_everything(random_seed)\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import (Dataset, DataLoader)\n",
    "\n",
    "\n",
    "#basic libs\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "import gc\n",
    "import copy\n",
    "\n",
    "#additional data processing\n",
    "\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "\n",
    "#visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#load images\n",
    "import matplotlib.image as mpimg\n",
    "import PIL\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#settings\n",
    "pd.options.display.max_rows = 100\n",
    "pd.options.display.max_columns = 100\n",
    "\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "random_seed=1234\n",
    "pl.seed_everything(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53389b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5e4cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_folder = 'images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e6aabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = f'{img_folder}/777311_0.png' \n",
    "# img_path = f'{img_folder}/006388_0.png'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1738077e",
   "metadata": {},
   "source": [
    "### Get image metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e78521",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the file info\n",
    "Path(img_path).stat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7f56aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get image meta data using pillow\n",
    "#https://pillow.readthedocs.io/en/stable/reference/Image.html?highlight=attributes#image-attributes\n",
    "\n",
    "img = Image.open(img_path)\n",
    "\n",
    "meta_dict = {    \n",
    "            'filename': img.filename,\n",
    "            'format': img.format, \n",
    "            'mode': img.mode,  \n",
    "            'size': img.size,  #2-tuple (width, height).\n",
    "\n",
    "            'width': img.width, \n",
    "            'height': img.height, \n",
    "            'palette': img.palette, \n",
    "            'info': img.info, \n",
    "            'is_animated': img.is_animated, \n",
    "            'n_frames': img.n_frames, \n",
    "}\n",
    "\n",
    "img.close()\n",
    "del img\n",
    "gc.collect()\n",
    "\n",
    "meta_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237355c2",
   "metadata": {},
   "source": [
    "### Load and resize images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5ce7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "img = Image.open(img_path)\n",
    "print(img.size)\n",
    "# img = np.asarray(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f8e651",
   "metadata": {},
   "outputs": [],
   "source": [
    "#display the original image\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd300a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/71738218/module-pil-has-not-attribute-resampling\n",
    "#dealing with pillow version differences\n",
    "print(PIL.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5786d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the thumbnail of the image\n",
    "\n",
    "if hasattr(Image, 'Resampling'):  # Pillow<8.4.0\n",
    "    PIL.Image.Resampling = PIL.Image\n",
    "    img.thumbnail((1024, 1024), resample=Image.Resampling.LANCZOS, reducing_gap=10)\n",
    "    if (img.height> img.width):\n",
    "        img = img.transpose(PIL.Image.Transpose.ROTATE_90)\n",
    "else:\n",
    "    img.thumbnail((1024, 1024), resample=Image.LANCZOS, reducing_gap=10)\n",
    "    if (img.height> img.width):\n",
    "        img = img.transpose(PIL.Image.ROTATE_90)\n",
    "    \n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0982c1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.asarray(img, np.uint8).min(), np.asarray(img, np.uint8).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4d3189",
   "metadata": {},
   "source": [
    "### Crop and pad images by torchvisaion tranforms\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33cae73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/10965417/how-to-convert-a-numpy-array-to-pil-image-applying-matplotlib-colormap\n",
    "\n",
    "#use torchvision to center crop the image\n",
    "img2 = transforms.functional.center_crop(img, 1024)\n",
    "print(img2.size)\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(img2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659fe6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.asarray(img2, np.uint8).min(), np.asarray(img2, np.uint8).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964273b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "img3 = transforms.functional.pad(img, 10)\n",
    "print(img3.size)\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(img3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ebe553",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.asarray(img3, np.uint8).min(), np.asarray(img3, np.uint8).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a088da",
   "metadata": {},
   "source": [
    "### Add Gaussian Blur to images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842254c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "img4 = transforms.functional.gaussian_blur(img, kernel_size=(5, 9), sigma=(0.1, 5))\n",
    "print(img4.size)\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(img4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1bdcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.asarray(img4, np.uint8).min(), np.asarray(img4, np.uint8).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d804a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "img4 = transforms.functional.gaussian_blur(img2, kernel_size=(5, 9), sigma=(0.1, 5))\n",
    "print(img4.size)\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(img4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6be997",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.asarray(img4, np.uint8).min(), np.asarray(img4, np.uint8).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64af9609",
   "metadata": {},
   "source": [
    "### Normalized image\n",
    "\n",
    "- to apply the `torchvisaion` transforms normalize function:\n",
    "    - first convert the `PIL` image object into numpy array (the data range is `0, 255]`)\n",
    "    - then reshape the numpy array from height*width*channels (for rgb images, the number of channels is 3) to channels*height*width\n",
    "    - make the data range from `[0, 255]` to `[0. 1]`\n",
    "    - normalize the data using `torchvision` *transforms.functional.normalize*\n",
    "    - reshape the numpy back to height*width*channels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89eff84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "img5 = np.asarray(img)\n",
    "print(img5.shape)\n",
    "print(img5.min(), img5.max())\n",
    "img5 = img5.transpose((2,0,1))\n",
    "print(img5.shape)\n",
    "img5 = img5/255\n",
    "print(img5.min(), img5.max())\n",
    "#make sure the array is normalized to 0-1 before applying normalize\n",
    "img5 = transforms.functional.normalize(torch.Tensor(img5), [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "img5 = img5.numpy().transpose((1,2,0))\n",
    "print(img5.min(), img5.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de98f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(img5)\n",
    "# plt.imshow(Image.fromarray(np.uint8(img5)*255))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bd8a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(np.clip(img5, 0, 1))\n",
    "# plt.imshow(Image.fromarray(np.uint8(img5)*255))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a467f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "img5_1 = img/np.amax(img5) # if float\n",
    "img5_1 = np.array(img5_1/np.amax(img5_1)*255, np.int32) # if int\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(img5_1)\n",
    "# plt.imshow(Image.fromarray(np.uint8(img5)*255))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58057e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.uint8(img5)*255).min(), (np.uint8(img5)*255).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ee79b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "# plt.imshow(img5)\n",
    "plt.imshow(Image.fromarray(np.uint8(img5)*255))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57512775",
   "metadata": {},
   "outputs": [],
   "source": [
    "img5 = np.asarray(img2)\n",
    "print(img5.shape)\n",
    "\n",
    "img5 = img5.transpose((2,0,1))\n",
    "img5 = img5/255\n",
    "img5 = transforms.functional.normalize(torch.Tensor(img5), [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "img5 = img5.numpy().transpose((1,2,0))\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(img5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ccc841",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(img5.min(), img5.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74724012",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/code/jirkaborovec/bloodclots-eda-load-wsi-prune-background?scriptVersionId=101797769\n",
    "\n",
    "def prune_image_rows_cols(im, mask, thr=0.990):\n",
    "    # delete empty columns\n",
    "    for l in reversed(range(im.shape[1])):\n",
    "        if (np.sum(mask[:, l]) / float(mask.shape[0])) > thr:\n",
    "            im = np.delete(im, l, 1)\n",
    "    # delete empty rows\n",
    "    for l in reversed(range(im.shape[0])):\n",
    "        if (np.sum(mask[l, :]) / float(mask.shape[1])) > thr:\n",
    "            im = np.delete(im, l, 0)\n",
    "    return im\n",
    "\n",
    "\n",
    "def mask_median(im, val=255):\n",
    "    masks = [None] * 3\n",
    "    for c in range(3):\n",
    "        masks[c] = im[..., c] >= np.median(im[:, :, c]) - 5\n",
    "    mask = np.logical_and(*masks)\n",
    "    im[mask, :] = val\n",
    "    return im, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe31016",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(f'{img_folder}/777311_0.png')\n",
    "print(img.size)\n",
    "img, mask = mask_median(np.array(img))\n",
    "img = prune_image_rows_cols(img, mask)\n",
    "img = Image.fromarray(np.uint8(img))\n",
    "print(img.size)\n",
    "if (img.height> img.width):\n",
    "    img = img.transpose(PIL.Image.ROTATE_90)\n",
    "ratio = img.height/img.width\n",
    "img = img.resize((512, int(512*ratio)), resample=Image.LANCZOS, reducing_gap=10)\n",
    "print(img.size)\n",
    "img = transforms.functional.center_crop(img, 512)\n",
    "img = transforms.functional.gaussian_blur(img, kernel_size=(5, 9), sigma=(0.1, 5))\n",
    "\n",
    "\n",
    "img = np.asarray(img)\n",
    "print(img.shape)\n",
    "\n",
    "img = img.transpose((2,0,1))\n",
    "img = img/255\n",
    "print(img.shape)\n",
    "img = transforms.functional.normalize(torch.FloatTensor(img), [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "img = img.numpy().transpose((1,2,0))\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3744e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "img6, mask6 = mask_median(np.array(img))\n",
    "img6 = prune_image_rows_cols(img6, mask6)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(img6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949f0126",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770559ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507c7c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "img6.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf74ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(img6/255)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b821149",
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40445817",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = img.transpose((1,2,0))/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0637d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(img2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eab69ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c48e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.zeros((500, 500, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc192246",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms.functional.center_crop\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a04107",
   "metadata": {},
   "outputs": [],
   "source": [
    "center_crops = [T.CenterCrop(size=size)(orig_img) for size in (30, 50, 100, orig_img.size)]\n",
    "plot(center_crops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3794a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5f700c",
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape, img.transpose((2,0,1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50710eac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f77db0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# Visualize a few images\n",
    "# ^^^^^^^^^^^^^^^^^^^^^^\n",
    "# Let's visualize a few training images so as to understand the data\n",
    "# augmentations.\n",
    "\n",
    "def imshow(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "\n",
    "\n",
    "# Get a batch of training data\n",
    "inputs, classes = next(iter(dataloaders['train']))\n",
    "\n",
    "# Make a grid from batch\n",
    "out = torchvision.utils.make_grid(inputs)\n",
    "\n",
    "imshow(out, title=[class_names[x] for x in classes])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d3bed1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
